model_name_or_path: '/root/.cache/huggingface/hub/models--yejunliang23--ShapeLLM-7B-omni/snapshots/b9922e4feb9bfbffe5f1c9941146b8a8f1fb124e'
stage: 'sft'
do_train: true
finetuning_type: 'lora'
lora_target: 'all'
dataset: 'shapellm_bdf'
template: 'llama3'
cutoff_len: 4096
max_samples: 1000
overwrite_cache: true
preprocessing_num_workers: 16
output_dir: '/root/autodl-tmp/lora_output'
logging_steps: 5
save_steps: 100
plot_loss: true
overwrite_output_dir: true
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
dataloader_num_workers: 2
dataloader_pin_memory: false
max_grad_norm: 1.0
gradient_checkpointing: true
learning_rate: 0.0002
num_train_epochs: 10.0
lr_scheduler_type: 'cosine'
warmup_ratio: 0.1
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.00e-08
bf16: true
tf32: true
ddp_timeout: 180000000
include_num_input_tokens_seen: true
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
save_only_model: true
report_to: 'none'
run_name: 'shapellm_bdf_lora_finetune'
logging_first_step: true
save_total_limit: 3
logging_dir: '/root/autodl-tmp/lora_output/logs'