#!/usr/bin/env python3
"""
ShapeLLM LoRAå¾®è°ƒè„šæœ¬

å¤„ç†ç¯å¢ƒåˆ‡æ¢é—®é¢˜ï¼š
- ShapeLLM-Omniè¿è¡Œåœ¨shapeLLMç¯å¢ƒ
- llama-factoryè¿è¡Œåœ¨baseç¯å¢ƒ
"""

import os
import sys
import json
import subprocess
import argparse
from pathlib import Path
import shutil

class ShapeLLMLoRATrainer:
    def __init__(self):
        self.project_root = Path.cwd()
        self.config_dir = self.project_root / "llamafactory_configs"
        self.data_dir = self.project_root / "training_data"
        self.output_dir = self.project_root / "lora_output"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.config_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
    
    def check_environments(self):
        """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
        print("=== æ£€æŸ¥ç¯å¢ƒé…ç½® ===")
        
        # æ£€æŸ¥å½“å‰ç¯å¢ƒ
        current_env = os.environ.get('CONDA_DEFAULT_ENV', 'base')
        print(f"å½“å‰ç¯å¢ƒ: {current_env}")
        
        # æ£€æŸ¥llama-factoryæ˜¯å¦åœ¨baseç¯å¢ƒ
        try:
            result = subprocess.run(['which', 'llamafactory-cli'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ“ æ‰¾åˆ°llamafactory-cli: {result.stdout.strip()}")
            else:
                print("âœ— æœªæ‰¾åˆ°llamafactory-cli")
                return False
        except Exception as e:
            print(f"âœ— æ£€æŸ¥llamafactory-cliå¤±è´¥: {e}")
            return False
        
        return True
    
    def find_shapellm_model(self):
        """æŸ¥æ‰¾æœ¬åœ°ShapeLLMæ¨¡å‹"""
        print("=== æŸ¥æ‰¾ShapeLLMæ¨¡å‹ ===")
        
        cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
        shapellm_dirs = [d for d in cache_dir.iterdir() if "ShapeLLM" in d.name]
        
        if shapellm_dirs:
            shapellm_dir = shapellm_dirs[0]
            snapshots_dir = shapellm_dir / "snapshots"
            if snapshots_dir.exists():
                snapshots = list(snapshots_dir.iterdir())
                if snapshots:
                    model_path = snapshots[0]
                    print(f"âœ“ æ‰¾åˆ°ShapeLLMæ¨¡å‹: {model_path}")
                    return str(model_path)
        
        print("âœ— æœªæ‰¾åˆ°æœ¬åœ°ShapeLLMæ¨¡å‹")
        return None
    
    def create_dataset_config(self, data_file):
        """åˆ›å»ºæ•°æ®é›†é…ç½®æ–‡ä»¶"""
        print("=== åˆ›å»ºæ•°æ®é›†é…ç½® ===")

        # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(data_file).exists():
            print(f"âœ— æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return False

        # åœ¨å½“å‰é¡¹ç›®ç›®å½•åˆ›å»ºdataç›®å½•
        data_dir = self.project_root / "data"
        data_dir.mkdir(exist_ok=True)

        # å¤åˆ¶è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶å
        data_filename = Path(data_file).name
        target_data_file = data_dir / data_filename
        shutil.copy2(data_file, target_data_file)
        print(f"âœ“ è®­ç»ƒæ•°æ®å¤åˆ¶åˆ°: {target_data_file}")

        # åˆ›å»ºdataset_info.json - ä½¿ç”¨LlamaFactoryå…¼å®¹æ ¼å¼
        dataset_info = {
            "shapellm_bdf": {
                "file_name": data_filename,
                "formatting": "sharegpt",
                "columns": {
                    "messages": "messages"
                }
            }
        }

        dataset_info_file = data_dir / "dataset_info.json"
        with open(dataset_info_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, indent=2, ensure_ascii=False)

        print(f"âœ“ æ•°æ®é›†é…ç½®åˆ›å»º: {dataset_info_file}")
        return True
    
    def create_training_config(self, model_path):
        """åˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶ - ä¼˜åŒ–48GBæ˜¾å­˜ä½¿ç”¨å’Œwandbé›†æˆ"""
        print("=== åˆ›å»ºè®­ç»ƒé…ç½® ===")

        config = {
            # æ¨¡å‹å’Œæ•°æ®é…ç½®
            "model_name_or_path": model_path,
            "stage": "sft",
            "do_train": True,
            "finetuning_type": "lora",
            "lora_target": "all",
            "dataset": "shapellm_bdf",
            "template": "llama3",
            "cutoff_len": 4096,
            "max_samples": 1000,
            "overwrite_cache": True,
            "preprocessing_num_workers": 16,

            # è¾“å‡ºå’Œæ—¥å¿—é…ç½®
            "output_dir": str(self.output_dir),
            "logging_steps": 5,
            "save_steps": 100,
            "plot_loss": True,
            "overwrite_output_dir": True,

            # æ˜¾å­˜ä¼˜åŒ–é…ç½® (ä¿å®ˆè®¾ç½®)
            "per_device_train_batch_size": 1,  # æœ€å°batch size
            "gradient_accumulation_steps": 4, # å¢åŠ æ¢¯åº¦ç´¯ç§¯ä¿æŒæœ‰æ•ˆbatch size
            "dataloader_num_workers": 2,       # å‡å°‘å·¥ä½œè¿›ç¨‹
            "dataloader_pin_memory": False,    # ç¦ç”¨å†…å­˜å›ºå®š
            "max_grad_norm": 1.0,              # æ¢¯åº¦è£å‰ª
            "gradient_checkpointing": True,    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜

            # å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨é…ç½®
            "learning_rate": 2e-4,              # ç¨å¾®æé«˜å­¦ä¹ ç‡
            "num_train_epochs": 10.0,            # å¢åŠ è®­ç»ƒè½®æ•°
            "lr_scheduler_type": "cosine",
            "warmup_ratio": 0.1,
            "weight_decay": 0.01,
            "adam_beta1": 0.9,
            "adam_beta2": 0.999,
            "adam_epsilon": 0.00000001,         # 1e-8 as float

            # ç²¾åº¦å’Œæ€§èƒ½é…ç½®
            "bf16": True,
            "tf32": True,                       # å¯ç”¨TF32åŠ é€Ÿ
            "ddp_timeout": 180000000,
            "include_num_input_tokens_seen": True,

            # LoRAé…ç½® (ä¿å®ˆè®¾ç½®èŠ‚çœæ˜¾å­˜)
            "lora_rank": 8,                     # å‡å°‘LoRA rankèŠ‚çœæ˜¾å­˜
            "lora_alpha": 16,                   # ç›¸åº”è°ƒæ•´alpha
            "lora_dropout": 0.05,
            "save_only_model": True,

            # æœ¬åœ°ç›‘æ§ (ä¸ä½¿ç”¨wandb)
            "report_to": "none",  # ç¦ç”¨wandb
            "run_name": "shapellm_bdf_lora_finetune",
            "logging_first_step": True,
            "save_total_limit": 3,
            "logging_dir": str(self.output_dir / "logs"),  # æœ¬åœ°æ—¥å¿—ç›®å½•
        }
        
        config_file = self.config_dir / "train_shapellm_lora.yaml"
        
        # è½¬æ¢ä¸ºYAMLæ ¼å¼
        yaml_content = []
        for key, value in config.items():
            if isinstance(value, str):
                yaml_content.append(f"{key}: '{value}'")
            elif isinstance(value, bool):
                yaml_content.append(f"{key}: {str(value).lower()}")
            elif isinstance(value, float):
                # ç¡®ä¿æµ®ç‚¹æ•°æ ¼å¼æ­£ç¡®
                if value < 1e-6:
                    yaml_content.append(f"{key}: {value:.2e}")  # ç§‘å­¦è®¡æ•°æ³•
                else:
                    yaml_content.append(f"{key}: {value}")
            else:
                yaml_content.append(f"{key}: {value}")
        
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(yaml_content))
        
        print(f"âœ“ è®­ç»ƒé…ç½®åˆ›å»º: {config_file}")
        return str(config_file)
    
    def run_lora_training(self, config_file):
        """è¿è¡ŒLoRAè®­ç»ƒ"""
        print("=== å¼€å§‹LoRAè®­ç»ƒ ===")

        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = '0'  # ç¡®ä¿ä½¿ç”¨GPU 0
        env['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerè­¦å‘Š

        # ç¡®ä¿åœ¨baseç¯å¢ƒä¸­è¿è¡Œ
        cmd = [
            'llamafactory-cli', 'train', config_file
        ]

        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"å·¥ä½œç›®å½•: {self.project_root}")

        try:
            # ä½¿ç”¨å®æ—¶è¾“å‡ºï¼Œè®¾ç½®æ­£ç¡®çš„å·¥ä½œç›®å½•
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1,
                cwd=str(self.project_root),  # è®¾ç½®å·¥ä½œç›®å½•
                env=env
            )

            # å®æ—¶æ˜¾ç¤ºè¾“å‡º
            for line in process.stdout:
                print(line.rstrip())

            process.wait()

            if process.returncode == 0:
                print("âœ“ LoRAè®­ç»ƒå®Œæˆ")
                return True
            else:
                print(f"âœ— LoRAè®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {process.returncode}")
                return False

        except Exception as e:
            print(f"âœ— è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def run_full_pipeline(self, data_file):
        """è¿è¡Œå®Œæ•´çš„LoRAå¾®è°ƒæµç¨‹"""
        print("=== ShapeLLM LoRAå¾®è°ƒæµç¨‹ ===\n")
        
        # æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒ
        if not self.check_environments():
            return False
        
        # æ­¥éª¤2: æŸ¥æ‰¾æ¨¡å‹
        model_path = self.find_shapellm_model()
        if not model_path:
            return False
        
        # æ­¥éª¤3: åˆ›å»ºæ•°æ®é›†é…ç½®
        if not self.create_dataset_config(data_file):
            return False
        
        # æ­¥éª¤4: åˆ›å»ºè®­ç»ƒé…ç½®
        config_file = self.create_training_config(model_path)
        if not config_file:
            return False
        
        # æ­¥éª¤5: è¿è¡Œè®­ç»ƒ
        if not self.run_lora_training(config_file):
            return False
        
        print("\nğŸ‰ LoRAå¾®è°ƒå®Œæˆ!")
        print(f"æ¨¡å‹ä¿å­˜åœ¨: {self.output_dir}")
        return True

def main():
    parser = argparse.ArgumentParser(description="ShapeLLM LoRAå¾®è°ƒ")
    parser.add_argument("--data_file", type=str, required=True,
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.data_file).exists():
        print(f"âœ— è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_file}")
        sys.exit(1)
    
    # è¿è¡ŒLoRAå¾®è°ƒ
    trainer = ShapeLLMLoRATrainer()
    success = trainer.run_full_pipeline(args.data_file)
    
    if success:
        print("\nğŸ‰ LoRAå¾®è°ƒæˆåŠŸå®Œæˆ!")
        sys.exit(0)
    else:
        print("\nğŸ’¥ LoRAå¾®è°ƒå¤±è´¥!")
        sys.exit(1)

if __name__ == "__main__":
    main()
